# spike-driven transformer论文代码复现

### 1. model部分的hook的作用



### 2. Conv2d中bias=false和true的区别



### 3. timm.DropPath的使用和Dropout的区别

- dropout的作用是将变量x中的某些值置为0，这样在下一个神经元的连接上该点的连接就被drop掉了
- DropPath的作用是随机的将一个batch中的一部分sample整体置为0



### 4. talking heads attention的具体过程

talking-heads attention的论文：[Talking-Heads Attention](https://arxiv.org/abs/2003.02436)，[博客](https://blog.csdn.net/u012856866/article/details/120200861)

对于multi-head，$Q^{h},K^{h},V^h$分别是第$h$个head的矩阵，对一般的attention机制，是分别计算$O^{h}$作为head的输出，talking-head的做法是在不同head之间先进行注意力权重的混合，即：
$$
\hat{J}^{1}=Q^1{K^1}^{\top},\dots,\hat{J}^{H}=Q^H{K^H}^{\top}
$$

$$
\begin{pmatrix}
J^{1} \\
\vdots \\
J^{H}
\end{pmatrix}=\left(\begin{array}{cccc}
\lambda_{11} & \lambda_{12} & \cdots & \lambda_{1 h} \\
\lambda_{21} & \lambda_{22} & \cdots & \lambda_{2 h} \\
\vdots & \vdots & \ddots & \vdots \\
\lambda_{h 1} & \lambda_{h 2} & \cdots & \lambda_{h h}
\end{array}\right)\begin{pmatrix}
\hat{J}^{1} \\
\vdots \\
\hat{J}^{H}
\end{pmatrix}
$$

$$
P^1 = \text{softmax}\left(J^1 \right),dots,P^H = \text{softmax}\left(J^H \right)
$$

$$
O^1 =P^1V^1,\dots,O^H =P^HV^H
$$

$$
O=\left[O^1,O^2,\cdots,O^H \right]
$$

softmax包含除以$\sqrt{d_k}$的操作，最后再将$H$个输出拼接成$d_k$维的向量，由于这里引入了投影矩阵$\left\{\lambda_{ij} \right\}|_{H\times M}$，$M$不一定等于$H$，也即multi-heads数不一定等于$$



